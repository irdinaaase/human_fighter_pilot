{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set global random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"STTHK3013_pilot_performance_simulation_data.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values using median imputation\n",
    "df.fillna(df.median(), inplace=True)\n",
    "\n",
    "# Feature Engineering: Add Interaction and Polynomial Features\n",
    "df['reaction_stress_interaction'] = df['time_reaction'] * df['environmental_stressors']\n",
    "df['fatigue_mission_ratio'] = df['fatigue_level'] / (df['mission_complexity'] + 1)\n",
    "df['heart_rate_squared'] = df['heart_rate'] ** 2\n",
    "\n",
    "# Reclassify 'final_performance' into three categories\n",
    "def classify_performance(value):\n",
    "    if value in [0, 1]:\n",
    "        return 0  # Basic\n",
    "    elif value in [2, 3]:\n",
    "        return 1  # Skilled\n",
    "    else:\n",
    "        return 2  # Expert\n",
    "\n",
    "df['final_performance'] = df['final_performance'].apply(classify_performance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features sorted by Chi-Square p-values :\n",
      "\n",
      "                        Feature  Chi-Square p-value\n",
      "7                  stress_level            0.997657\n",
      "1                 sleep_quality            0.995977\n",
      "10        fatigue_mission_ratio            0.991588\n",
      "4       environmental_stressors            0.958933\n",
      "6                 fatigue_level            0.946188\n",
      "9   reaction_stress_interaction            0.928008\n",
      "0                    heart_rate            0.923088\n",
      "11           heart_rate_squared            0.922239\n",
      "5               cognitive_level            0.917228\n",
      "8                 time_reaction            0.899806\n",
      "3              experience_level            0.842575\n",
      "2            mission_complexity            0.837363\n",
      "\n",
      "Selected features based on Chi-Square test:\n",
      " 0                 heart_rate\n",
      "1              sleep_quality\n",
      "2         mission_complexity\n",
      "3           experience_level\n",
      "4    environmental_stressors\n",
      "5            cognitive_level\n",
      "dtype: object \n"
     ]
    }
   ],
   "source": [
    "# Use the cleaned data (data_cleaned) for Chi-Square test\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "X = df.drop(columns=['final_performance'])  # Correct column name\n",
    "y = df['final_performance']  # Correct column name\n",
    "\n",
    "# Remove Outliers using Z-score\n",
    "z_scores = np.abs(stats.zscore(X))\n",
    "X_filtered = X[(z_scores < 3).all(axis=1)]\n",
    "y_filtered = y[X_filtered.index]\n",
    "\n",
    "# Scale the feature data to be non-negative (using MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_filtered)\n",
    "\n",
    "# Apply Chi-Square Test for feature selection\n",
    "chi2_selector = SelectKBest(chi2, k='all')  # Select all features\n",
    "X_chi2 = chi2_selector.fit_transform(X_scaled, y_filtered)\n",
    "\n",
    "# Get the Chi-Square p-values for each feature\n",
    "p_values = chi2_selector.pvalues_\n",
    "\n",
    "# Create a DataFrame to sort the features by p-values\n",
    "p_values_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Chi-Square p-value': p_values\n",
    "})\n",
    "\n",
    "# Sort the features by p-value (ascending order)\n",
    "p_values_df_sorted = p_values_df.sort_values(by='Chi-Square p-value', ascending=False)\n",
    "\n",
    "# Print the sorted features by their Chi-Square p-value\n",
    "print(\"\\nFeatures sorted by Chi-Square p-values :\\n\")\n",
    "print(p_values_df_sorted)\n",
    "\n",
    "# Display the selected features based on Chi-Square test\n",
    "selected_features = pd.Series(X.columns[chi2_selector.get_support()]).head(6)\n",
    "print(f\"\\nSelected features based on Chi-Square test:\\n {selected_features} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({2: 358, 1: 324, 0: 297})\n",
      "Class distribution after SMOTE: Counter({1: 358, 2: 358, 0: 358})\n",
      "Original data size: 979\n",
      "Resampled data size: 1074\n",
      "Percentage increase in data size: 9.70%\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Subset the dataset with the top 5 features and the numerical target\n",
    "selected_features = [            \n",
    "              'heart_rate',\n",
    "              'sleep_quality',\n",
    "              'mission_complexity',\n",
    "              'experience_level',\n",
    "              'environmental_stressors',\n",
    "              'cognitive_level',\n",
    "]\n",
    "X = df[selected_features]\n",
    "y = df['final_performance']  # Numerical target\n",
    "\n",
    "# Display original class distribution\n",
    "print(f\"Original class distribution: {Counter(y)}\")\n",
    "\n",
    "# Calculate the desired increase in data size (20% increase)\n",
    "original_size = len(X)\n",
    "desired_size = int(original_size * 1.177)  # 20% increase\n",
    "\n",
    "# Calculate the sampling strategy for SMOTE\n",
    "majority_class = max(Counter(y).values())\n",
    "desired_samples_per_class = int(desired_size / len(Counter(y)))\n",
    "sampling_strategy = {cls: max(min(desired_samples_per_class, majority_class), Counter(y)[cls]) for cls in Counter(y).keys()}\n",
    "\n",
    "# Apply SMOTE with the calculated sampling strategy\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Display class distribution after SMOTE\n",
    "print(f\"Class distribution after SMOTE: {Counter(y_resampled)}\")\n",
    "\n",
    "# Display original and resampled data sizes\n",
    "print(f\"Original data size: {len(X)}\")\n",
    "print(f\"Resampled data size: {len(X_resampled)}\")\n",
    "print(f\"Percentage increase in data size: {((len(X_resampled) - len(X)) / len(X)) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\miniconda3\\lib\\site-packages\\xgboost\\compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Logistic Regression...\n",
      "Best Params for Logistic Regression: {'C': 0.1, 'solver': 'liblinear'}\n",
      "Optimizing Deep Neural Network (MLP)...\n",
      "Best Params for Deep Neural Network (MLP): {'activation': 'tanh', 'hidden_layer_sizes': (512, 256, 128), 'learning_rate_init': 0.001}\n",
      "Optimizing XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\miniconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\USER\\miniconda3\\lib\\site-packages\\xgboost\\data.py:192: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:43:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best Params for XGBoost: {'learning_rate': 0.05, 'max_depth': 9, 'n_estimators': 500}\n",
      "===== Logistic Regression =====\n",
      "Accuracy: 0.37209302325581395\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.38      0.39        71\n",
      "           1       0.44      0.39      0.41        72\n",
      "           2       0.30      0.35      0.32        72\n",
      "\n",
      "    accuracy                           0.37       215\n",
      "   macro avg       0.38      0.37      0.37       215\n",
      "weighted avg       0.38      0.37      0.37       215\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[27 12 32]\n",
      " [19 28 25]\n",
      " [23 24 25]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\miniconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Deep Neural Network (MLP) =====\n",
      "Accuracy: 0.4558139534883721\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.52      0.51        71\n",
      "           1       0.42      0.44      0.43        72\n",
      "           2       0.45      0.40      0.42        72\n",
      "\n",
      "    accuracy                           0.46       215\n",
      "   macro avg       0.46      0.46      0.46       215\n",
      "weighted avg       0.46      0.46      0.46       215\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[37 18 16]\n",
      " [20 32 20]\n",
      " [16 27 29]] \n",
      "\n",
      "[01:43:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "===== XGBoost =====\n",
      "Accuracy: 0.3767441860465116\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.52      0.48        71\n",
      "           1       0.29      0.32      0.30        72\n",
      "           2       0.39      0.29      0.33        72\n",
      "\n",
      "    accuracy                           0.38       215\n",
      "   macro avg       0.38      0.38      0.37       215\n",
      "weighted avg       0.38      0.38      0.37       215\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[37 22 12]\n",
      " [28 23 21]\n",
      " [17 34 21]] \n",
      "\n",
      "\n",
      "Best Model: Deep Neural Network (MLP) with Accuracy: 0.4558\n",
      "Best Model Parameters: {'activation': 'tanh', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (512, 256, 128), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 4: Train and Optimize Models with Stratified K-Fold CV\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['lbfgs', 'liblinear']\n",
    "    },\n",
    "    \"Deep Neural Network (MLP)\": {\n",
    "        'hidden_layer_sizes': [(256, 128, 64), (512, 256, 128)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'learning_rate_init': [0.001, 0.005]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        'n_estimators': [300, 400, 500],\n",
    "        'max_depth': [6, 9, 12],\n",
    "        'learning_rate': [0.01, 0.05, 0.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=RANDOM_STATE),\n",
    "    \"Deep Neural Network (MLP)\": MLPClassifier(random_state=RANDOM_STATE),\n",
    "    \"XGBoost\": XGBClassifier(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "best_models = {}\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for name, param_grid in param_grids.items():\n",
    "    print(f\"Optimizing {name}...\")\n",
    "    grid_search = GridSearchCV(models[name], param_grid, cv=skf, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    print(f\"Best Params for {name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Split dataset for final testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=RANDOM_STATE, stratify=y_resampled)\n",
    "\n",
    "# Standardize features using RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Train & Compare All Optimized Models\n",
    "final_results = {}\n",
    "for name, model in best_models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, digits=2)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    final_results[name] = {\"accuracy\": accuracy, \"report\": report, \"confusion_matrix\": conf_matrix}\n",
    "\n",
    "    # Display results\n",
    "    print(f\"===== {name} =====\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "    print(\"\\nConfusion Matrix:\\n\", conf_matrix, \"\\n\")\n",
    "\n",
    "# Identify the Best Performing Model\n",
    "best_model_name = max(final_results, key=lambda x: final_results[x]['accuracy'])\n",
    "best_model_params = best_models[best_model_name].get_params()\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name} with Accuracy: {final_results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"Best Model Parameters: {best_model_params}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
