{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set global random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"STTHK3013_pilot_performance_simulation_data.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values using median imputation\n",
    "df.fillna(df.median(), inplace=True)\n",
    "\n",
    "# Feature Engineering: Add Interaction and Polynomial Features\n",
    "df['reaction_stress_interaction'] = df['time_reaction'] * df['environmental_stressors']\n",
    "df['fatigue_mission_ratio'] = df['fatigue_level'] / (df['mission_complexity'] + 1)\n",
    "df['heart_rate_squared'] = df['heart_rate'] ** 2\n",
    "\n",
    "# Reclassify 'final_performance' into three categories\n",
    "def classify_performance(value):\n",
    "    if value in [0, 1]:\n",
    "        return 0  # Basic\n",
    "    elif value in [2, 3]:\n",
    "        return 1  # Skilled\n",
    "    else:\n",
    "        return 2  # Expert\n",
    "\n",
    "df['final_performance'] = df['final_performance'].apply(classify_performance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features sorted by Chi-Square p-values :\n",
      "\n",
      "                        Feature  Chi-Square p-value\n",
      "7                  stress_level            0.997657\n",
      "1                 sleep_quality            0.995977\n",
      "10        fatigue_mission_ratio            0.991588\n",
      "4       environmental_stressors            0.958933\n",
      "6                 fatigue_level            0.946188\n",
      "9   reaction_stress_interaction            0.928008\n",
      "0                    heart_rate            0.923088\n",
      "11           heart_rate_squared            0.922239\n",
      "5               cognitive_level            0.917228\n",
      "8                 time_reaction            0.899806\n",
      "3              experience_level            0.842575\n",
      "2            mission_complexity            0.837363\n",
      "\n",
      "Selected features based on Chi-Square test:\n",
      " 0                 heart_rate\n",
      "1              sleep_quality\n",
      "2         mission_complexity\n",
      "3           experience_level\n",
      "4    environmental_stressors\n",
      "5            cognitive_level\n",
      "dtype: object \n"
     ]
    }
   ],
   "source": [
    "# Use the cleaned data (data_cleaned) for Chi-Square test\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "X = df.drop(columns=['final_performance'])  # Correct column name\n",
    "y = df['final_performance']  # Correct column name\n",
    "\n",
    "# Remove Outliers using Z-score\n",
    "z_scores = np.abs(stats.zscore(X))\n",
    "X_filtered = X[(z_scores < 3).all(axis=1)]\n",
    "y_filtered = y[X_filtered.index]\n",
    "\n",
    "# Scale the feature data to be non-negative (using MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_filtered)\n",
    "\n",
    "# Apply Chi-Square Test for feature selection\n",
    "chi2_selector = SelectKBest(chi2, k='all')  # Select all features\n",
    "X_chi2 = chi2_selector.fit_transform(X_scaled, y_filtered)\n",
    "\n",
    "# Get the Chi-Square p-values for each feature\n",
    "p_values = chi2_selector.pvalues_\n",
    "\n",
    "# Create a DataFrame to sort the features by p-values\n",
    "p_values_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Chi-Square p-value': p_values\n",
    "})\n",
    "\n",
    "# Sort the features by p-value (ascending order)\n",
    "p_values_df_sorted = p_values_df.sort_values(by='Chi-Square p-value', ascending=False)\n",
    "\n",
    "# Print the sorted features by their Chi-Square p-value\n",
    "print(\"\\nFeatures sorted by Chi-Square p-values :\\n\")\n",
    "print(p_values_df_sorted)\n",
    "\n",
    "# Display the selected features based on Chi-Square test\n",
    "selected_features = pd.Series(X.columns[chi2_selector.get_support()]).head(6)\n",
    "print(f\"\\nSelected features based on Chi-Square test:\\n {selected_features} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({2: 358, 1: 324, 0: 297})\n",
      "Class distribution after SMOTE: Counter({1: 358, 2: 358, 0: 358})\n",
      "Original data size: 979\n",
      "Resampled data size: 1074\n",
      "Percentage increase in data size: 9.70%\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Subset the dataset with the top 5 features and the numerical target\n",
    "selected_features = [            \n",
    "              'heart_rate',\n",
    "              'sleep_quality',\n",
    "              'mission_complexity',\n",
    "              'experience_level',\n",
    "              'environmental_stressors',\n",
    "              'cognitive_level',\n",
    "]\n",
    "X = df[selected_features]\n",
    "y = df['final_performance']  # Numerical target\n",
    "\n",
    "# Display original class distribution\n",
    "print(f\"Original class distribution: {Counter(y)}\")\n",
    "\n",
    "# Calculate the desired increase in data size (20% increase)\n",
    "original_size = len(X)\n",
    "desired_size = int(original_size * 1.177)  # 20% increase\n",
    "\n",
    "# Calculate the sampling strategy for SMOTE\n",
    "majority_class = max(Counter(y).values())\n",
    "desired_samples_per_class = int(desired_size / len(Counter(y)))\n",
    "sampling_strategy = {cls: max(min(desired_samples_per_class, majority_class), Counter(y)[cls]) for cls in Counter(y).keys()}\n",
    "\n",
    "# Apply SMOTE with the calculated sampling strategy\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Display class distribution after SMOTE\n",
    "print(f\"Class distribution after SMOTE: {Counter(y_resampled)}\")\n",
    "\n",
    "# Display original and resampled data sizes\n",
    "print(f\"Original data size: {len(X)}\")\n",
    "print(f\"Resampled data size: {len(X_resampled)}\")\n",
    "print(f\"Percentage increase in data size: {((len(X_resampled) - len(X)) / len(X)) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_23440\\2981810604.py:19: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  dnn_model = KerasClassifier(build_fn=create_dnn_model, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "DNN Accuracy for Fold 1: 0.35\n",
      "\n",
      "Fold 2\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "DNN Accuracy for Fold 2: 0.40\n",
      "\n",
      "Fold 3\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "DNN Accuracy for Fold 3: 0.36\n",
      "\n",
      "Fold 4\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "DNN Accuracy for Fold 4: 0.38\n",
      "\n",
      "Fold 5\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "DNN Accuracy for Fold 5: 0.37\n",
      "\n",
      "Mean DNN Accuracy across all folds: 0.37\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define DNN model creation function\n",
    "def create_dnn_model(optimizer='adam', activation='relu', neurons=32):\n",
    "    model = Sequential([\n",
    "        Dense(neurons, input_dim=X_resampled.shape[1], activation=activation),  # First hidden layer\n",
    "        Dense(neurons // 2, activation=activation),  # Second hidden layer\n",
    "        Dense(len(np.unique(y_resampled)), activation='softmax')  # Output layer\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap DNN model with KerasClassifier\n",
    "dnn_model = KerasClassifier(build_fn=create_dnn_model, verbose=0)\n",
    "\n",
    "# Define parameter grid for DNN model\n",
    "dnn_param_grid = {\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [50, 100],\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'neurons': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# Define K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize scalers and result container\n",
    "scaler = StandardScaler()\n",
    "dnn_accuracies = []\n",
    "\n",
    "# Loop through each fold for DNN\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X_resampled, y_resampled)):\n",
    "    print(f\"\\nFold {fold + 1}\")\n",
    "\n",
    "    # Reset index of X_resampled to ensure indices match\n",
    "    X_resampled_reset = X_resampled.reset_index(drop=True)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X_resampled_reset.iloc[train_index], X_resampled_reset.iloc[test_index]\n",
    "    y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "    \n",
    "    # Scale the features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train the DNN model using GridSearchCV\n",
    "    dnn_grid_search = GridSearchCV(estimator=dnn_model, param_grid=dnn_param_grid, scoring='accuracy', cv=3, n_jobs=-1, verbose=1)\n",
    "    dnn_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get the best DNN model and evaluate it\n",
    "    best_dnn_model = dnn_grid_search.best_estimator_\n",
    "    y_pred_dnn = best_dnn_model.predict(X_test_scaled)\n",
    "    dnn_accuracy = accuracy_score(y_test, y_pred_dnn)\n",
    "    print(f\"DNN Accuracy for Fold {fold + 1}: {dnn_accuracy:.2f}\")\n",
    "    dnn_accuracies.append(dnn_accuracy)\n",
    "\n",
    "# Calculate and display the mean accuracy for DNN\n",
    "mean_dnn_accuracy = np.mean(dnn_accuracies)\n",
    "print(f\"\\nMean DNN Accuracy across all folds: {mean_dnn_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "[01:09:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\miniconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy for Fold 1: 0.44\n",
      "\n",
      "Fold 2\n",
      "[01:09:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\miniconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy for Fold 2: 0.37\n",
      "\n",
      "Fold 3\n",
      "[01:09:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\miniconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy for Fold 3: 0.35\n",
      "\n",
      "Fold 4\n",
      "[01:09:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\miniconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy for Fold 4: 0.35\n",
      "\n",
      "Fold 5\n",
      "[01:09:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\miniconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy for Fold 5: 0.39\n",
      "\n",
      "Mean XGBoost Accuracy across all folds: 0.38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Initialize scalers and result container\n",
    "scaler = StandardScaler()\n",
    "xgb_accuracies = []\n",
    "\n",
    "# Define the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=RANDOM_STATE)\n",
    "\n",
    "# Convert X_resampled to DataFrame to use reset_index\n",
    "X_resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "\n",
    "# Loop through each fold for XGBoost\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X_resampled_df, y_resampled)):\n",
    "    print(f\"\\nFold {fold + 1}\")\n",
    "\n",
    "    # Reset index of X_resampled to ensure indices match\n",
    "    X_resampled_reset = X_resampled_df.reset_index(drop=True)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X_resampled_reset.iloc[train_index], X_resampled_reset.iloc[test_index]\n",
    "    y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "    \n",
    "    # Scale the features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    xgb_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "    xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "    print(f\"XGBoost Accuracy for Fold {fold + 1}: {xgb_accuracy:.2f}\")\n",
    "    xgb_accuracies.append(xgb_accuracy)\n",
    "\n",
    "# Calculate and display the mean accuracy for XGBoost\n",
    "mean_xgb_accuracy = np.mean(xgb_accuracies)\n",
    "print(f\"\\nMean XGBoost Accuracy across all folds: {mean_xgb_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Logistic Regression Accuracy for Fold 1: 0.33\n",
      "\n",
      "Fold 2\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Logistic Regression Accuracy for Fold 2: 0.35\n",
      "\n",
      "Fold 3\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Logistic Regression Accuracy for Fold 3: 0.34\n",
      "\n",
      "Fold 4\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Logistic Regression Accuracy for Fold 4: 0.33\n",
      "\n",
      "Fold 5\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Logistic Regression Accuracy for Fold 5: 0.34\n",
      "\n",
      "Mean Logistic Regression Accuracy across all folds: 0.34\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define Logistic Regression model\n",
    "log_reg_model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=5000)\n",
    "\n",
    "# Define parameter grid for Logistic Regression\n",
    "log_reg_param_grid = {\n",
    "    'C': [0.1, 1, 10],                # Regularization strength\n",
    "    'solver': ['lbfgs', 'newton-cg'], # Solvers to try\n",
    "}\n",
    "\n",
    "# Define K-Fold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize scalers and result container\n",
    "scaler = StandardScaler()\n",
    "log_reg_accuracies = []\n",
    "\n",
    "# Loop through each fold for Logistic Regression\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X_resampled, y_resampled)):\n",
    "    print(f\"\\nFold {fold + 1}\")\n",
    "\n",
    "    # Reset index of X_resampled to ensure indices match\n",
    "    X_resampled_reset = X_resampled.reset_index(drop=True)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X_resampled_reset.iloc[train_index], X_resampled_reset.iloc[test_index]\n",
    "    y_train, y_test = y_resampled[train_index], y_resampled[test_index]\n",
    "    \n",
    "    # Scale the features\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Train the Logistic Regression model using GridSearchCV\n",
    "    log_reg_grid_search = GridSearchCV(estimator=log_reg_model, param_grid=log_reg_param_grid, scoring='accuracy', cv=3, n_jobs=-1, verbose=1)\n",
    "    log_reg_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get the best Logistic Regression model and evaluate it\n",
    "    best_log_reg_model = log_reg_grid_search.best_estimator_\n",
    "    y_pred_log_reg = best_log_reg_model.predict(X_test_scaled)\n",
    "    log_reg_accuracy = accuracy_score(y_test, y_pred_log_reg)\n",
    "    print(f\"Logistic Regression Accuracy for Fold {fold + 1}: {log_reg_accuracy:.2f}\")\n",
    "    log_reg_accuracies.append(log_reg_accuracy)\n",
    "\n",
    "# Calculate and display the mean accuracy for Logistic Regression\n",
    "mean_log_reg_accuracy = np.mean(log_reg_accuracies)\n",
    "print(f\"\\nMean Logistic Regression Accuracy across all folds: {mean_log_reg_accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
