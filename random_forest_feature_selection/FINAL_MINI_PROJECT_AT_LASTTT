import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from collections import Counter
from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import RobustScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.feature_selection import RFECV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Set global random state for reproducibility
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
tf.random.set_seed(RANDOM_STATE)

# Load dataset
file_path = "STTHK3013_pilot_performance_simulation_data.xlsx"
df = pd.read_excel(file_path, sheet_name='Sheet1')

# Handle missing values using median imputation
df.fillna(df.median(), inplace=True)

# Feature Engineering: Add Interaction and Polynomial Features
df['reaction_stress_interaction'] = df['time_reaction'] * df['environmental_stressors']
df['fatigue_mission_ratio'] = df['fatigue_level'] / (df['mission_complexity'] + 1)
df['heart_rate_squared'] = df['heart_rate'] ** 2

# Reclassify 'final_performance' into three categories
def classify_performance(value):
    if value in [0, 1]:
        return 0  # Basic
    elif value in [2, 3]:
        return 1  # Skilled
    else:
        return 2  # Expert

df['final_performance'] = df['final_performance'].apply(classify_performance)

# Feature Selection using Recursive Feature Elimination (RFE)
X = df.drop(columns=['final_performance'])
y = df['final_performance']

# Step 1: Remove Outliers using Z-score
z_scores = np.abs(stats.zscore(X))
X_filtered = X[(z_scores < 3).all(axis=1)]
y_filtered = y[X_filtered.index]

# Step 2: Feature Selection using RFE + Cross-Validation
rf = RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)
rfecv = RFECV(estimator=rf, step=1, cv=5, scoring='accuracy')
rfecv.fit(X_filtered, y_filtered)
selected_features = X_filtered.columns[rfecv.support_].tolist()
print("Selected Features: ", selected_features)

# Train the model on the selected features
rf.fit(X_filtered[selected_features], y_filtered)

# Create a DataFrame for Feature Importances
importance_df = pd.DataFrame({
    'Feature (Random Forest)': selected_features,
    'Importance': rf.feature_importances_
})

# Sort by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Print the table in a clean format
print("\nFeature Importance from Random Forest:\n")
print(importance_df.to_string(index=False))

# Step 3: Apply SMOTE (20% Oversampling for Each Class)
class_distribution = Counter(y_filtered)
sampling_strategy = {cls: max(int(count * 1.2), count + 1) for cls, count in class_distribution.items()}
smote = SMOTE(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE)
X_resampled, y_resampled = smote.fit_resample(X_filtered[selected_features], y_filtered)

# Print class distributions before and after SMOTE
original_distribution = Counter(y_filtered)
new_distribution = Counter(y_resampled)
print("Original Class Distribution:", original_distribution)
print("New Class Distribution after SMOTE:", new_distribution)

# Define models
models = {
    "Random Forest": RandomForestClassifier(random_state=RANDOM_STATE),
    "Logistic Regression": LogisticRegression(max_iter=2000, random_state=RANDOM_STATE),
    "XGBoost": XGBClassifier(eval_metric='mlogloss', random_state=RANDOM_STATE)
}

# Define the Deep Neural Network (DNN) model
def create_dnn_model(input_dim, learning_rate=0.001):
    model = Sequential([
        Dense(256, activation='relu', input_shape=(input_dim,)),
        Dropout(0.3),
        Dense(128, activation='relu'),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dense(3, activation='softmax')  # 3 classes for classification
    ])
    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

# Step 4: Train and Optimize Models with Stratified K-Fold CV
param_grids = {
    "Random Forest": {
        'n_estimators': [300, 400, 500],
        'max_depth': [20, 30, 40, None],
        'min_samples_split': [2, 5, 10]
    },
    "Logistic Regression": {
        'C': [0.1, 1, 10],
        'solver': ['lbfgs', 'liblinear']
    },
    "XGBoost": {
        'n_estimators': [300, 400, 500],
        'max_depth': [6, 9, 12],
        'learning_rate': [0.01, 0.05, 0.1]
    }
}

best_models = {}
best_params = {}
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)

for name, param_grid in param_grids.items():
    print(f"Optimizing {name}...")
    grid_search = GridSearchCV(models[name], param_grid, cv=skf, scoring='accuracy', n_jobs=-1)
    grid_search.fit(X_resampled, y_resampled)
    
    best_models[name] = grid_search.best_estimator_
    best_params[name] = grid_search.best_params_
    print(f"Best Params for {name}: {grid_search.best_params_}")

# Split dataset for final testing
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=RANDOM_STATE, stratify=y_resampled)

# Standardize features using RobustScaler
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Deep Neural Network
input_dim = X_train_scaled.shape[1]
dnn_model = create_dnn_model(input_dim)

history = dnn_model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)

# Evaluate DNN Model
dnn_loss, dnn_accuracy = dnn_model.evaluate(X_test_scaled, y_test, verbose=0)
print(f"DNN Model Accuracy: {dnn_accuracy:.4f}")

# Save DNN Best Parameters
best_params["Deep Neural Network"] = {"learning_rate": 0.001, "epochs": 50, "batch_size": 32}

# Step 5: Train & Compare All Optimized Models
final_results = {}
for name, model in best_models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, digits=2)
    conf_matrix = confusion_matrix(y_test, y_pred)
    
    final_results[name] = {"accuracy": accuracy, "report": report, "confusion_matrix": conf_matrix}

    # Display results
    print(f"===== {name} =====")
    print("Accuracy:", accuracy)
    print("\nClassification Report:\n", report)
    print("\nConfusion Matrix:\n", conf_matrix, "\n")

# Add DNN Model Results
final_results["Deep Neural Network"] = {"accuracy": dnn_accuracy}

# Predict DNN Model on Test Set
y_pred_dnn = np.argmax(dnn_model.predict(X_test_scaled), axis=1)

# Compute Accuracy
dnn_accuracy = accuracy_score(y_test, y_pred_dnn)

# Generate Classification Report
dnn_report = classification_report(y_test, y_pred_dnn, digits=2)

# Generate Confusion Matrix
dnn_conf_matrix = confusion_matrix(y_test, y_pred_dnn)

# Print Results in the Same Format as Other Models
print("\n===== Deep Neural Network (DNN) =====")
print(f"Accuracy: {dnn_accuracy:.4f}")
print("\nClassification Report:\n", dnn_report)
print("\nConfusion Matrix:\n", dnn_conf_matrix, "\n")

# Add DNN Results to Final Results Dictionary
final_results["Deep Neural Network"] = {
    "accuracy": dnn_accuracy,
    "report": dnn_report,
    "confusion_matrix": dnn_conf_matrix
}


# Identify the Best Performing Model
best_model_name = max(final_results, key=lambda x: final_results[x]['accuracy'])
print(f"\nBest Model: {best_model_name} with Accuracy: {final_results[best_model_name]['accuracy']:.4f}")

# Print all best parameters for each model
print("\nBest Hyperparameters for Each Model:")
for model_name, params in best_params.items():
    print(f"{model_name}: {params}")
