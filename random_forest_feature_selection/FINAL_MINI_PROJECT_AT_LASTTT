import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from collections import Counter
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE

# Load dataset
file_path = "STTHK3013_pilot_performance_simulation_data.xlsx"
df = pd.read_excel(file_path, sheet_name='Sheet1')

# Handle missing values using median imputation
df.fillna(df.median(), inplace=True)

# Reclassify 'final_performance' into three categories
def classify_performance(value):
    if value in [0, 1]:
        return 0  # Basic
    elif value in [2, 3]:
        return 1  # Skilled
    else:
        return 2  # Expert

df['final_performance'] = df['final_performance'].apply(classify_performance)

# Feature Selection - Keeping all features (even if not used)
X = df.drop(columns=['final_performance'])
y = df['final_performance']

# Step 1: Remove Outliers using Z-score
z_scores = np.abs(stats.zscore(X))
X_filtered = X[(z_scores < 3).all(axis=1)]
y_filtered = y[X_filtered.index]

# Step 2: Feature Selection using Random Forest
rf = RandomForestClassifier(n_estimators=200, random_state=42)
rf.fit(X_filtered, y_filtered)

# Get feature importances
feature_importances = pd.Series(rf.feature_importances_, index=X_filtered.columns).sort_values(ascending=False)

# Select top features (keeping the top 6)
selected_features = feature_importances.head(6).index.tolist()
print("Selected Features: ", selected_features)

# Step 3: Apply SMOTE (only 20% oversampling)
smote_ratio = 'auto'  # 20% more data
smote = SMOTE(sampling_strategy=smote_ratio, random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_filtered[selected_features], y_filtered)

# Print class distributions before and after SMOTE
original_distribution = Counter(y_filtered)
new_distribution = Counter(y_resampled)
print("Original Class Distribution:", original_distribution)
print("New Class Distribution after 20% SMOTE:", new_distribution)

# Define models
models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Deep Neural Network (MLP)": MLPClassifier(hidden_layer_sizes=(64, 32), activation='relu', max_iter=500),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
}

# Step 4: Train and Optimize Models
param_grids = {
    "Random Forest": {
        'n_estimators': [200, 300, 400],
        'max_depth': [10, 20, 30, None],
        'min_samples_split': [2, 5, 10]
    },
    "Logistic Regression": {
        'C': [0.1, 1, 10],
        'solver': ['lbfgs', 'liblinear']
    },
    "Deep Neural Network (MLP)": {
        'hidden_layer_sizes': [(64, 32), (128, 64), (256, 128)],
        'activation': ['relu', 'tanh'],
        'learning_rate_init': [0.001, 0.01]
    },
    "XGBoost": {
        'n_estimators': [200, 300, 400],
        'max_depth': [3, 6, 9],
        'learning_rate': [0.01, 0.1, 0.2]
    }
}

best_models = {}
for name, param_grid in param_grids.items():
    print(f"Optimizing {name}...")
    grid_search = GridSearchCV(models[name], param_grid, cv=3, scoring='accuracy', n_jobs=-1)
    grid_search.fit(X_resampled, y_resampled)
    
    best_models[name] = grid_search.best_estimator_
    print(f"Best Params for {name}: {grid_search.best_params_}")

# Split dataset for final testing
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 5: Train & Compare All Optimized Models
final_results = {}
for name, model in best_models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, digits=2)
    conf_matrix = confusion_matrix(y_test, y_pred)
    
    final_results[name] = {"accuracy": accuracy, "report": report, "confusion_matrix": conf_matrix}

    # Display results like in the uploaded image
    print(f"===== {name} =====")
    print("Accuracy:", accuracy)
    print("\nClassification Report:\n", report)
    print("\nConfusion Matrix:\n", conf_matrix, "\n")

# Identify the Best Performing Model
best_model_name = max(final_results, key=lambda x: final_results[x]['accuracy'])
best_model_params = best_models[best_model_name].get_params()

print(f"\nBest Model: {best_model_name} with Accuracy: {final_results[best_model_name]['accuracy']:.4f}")
print(f"Best Model Parameters: {best_model_params}")